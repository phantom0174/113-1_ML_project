{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables 2\n",
    "\n",
    "goal: predict simple latex formula\n",
    "\n",
    "原始論文用 CNN，但暴力作法感覺不太可行，所以我改用 CNN + RNN，前者作為特徵擷取，後者用 LSTM。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./image_formula_mapping.json\", \"r\") as f:\n",
    "    img2labels: dict = json.load(f)\n",
    "\n",
    "with open(\"./archive/PRINTED_TEX_230k/230k.json\", \"r\") as f:\n",
    "    token2enc: dict = json.load(f)\n",
    "enc2token = { v: k for k, v in token2enc.items() }\n",
    "\n",
    "vocab_size = len(token2enc)\n",
    "\n",
    "loading = 0.05 # used training data for model testing, ~ 11.5k\n",
    "img2labels = dict(itertools.islice(\n",
    "    img2labels.items(), int(len(img2labels.keys()) * loading)\n",
    "))\n",
    "\n",
    "dataset = list(img2labels.keys()) # containing pic names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9372 2343\n"
     ]
    }
   ],
   "source": [
    "training_ratio = 0.8\n",
    "split_ind = int(len(dataset) * training_ratio)\n",
    "\n",
    "train_key = dataset[:split_ind]\n",
    "valid_key = dataset[split_ind:]\n",
    "\n",
    "train_data = dict(itertools.islice(img2labels.items(), split_ind))\n",
    "valid_data = dict(itertools.islice(img2labels.items(), split_ind, len(img2labels)))\n",
    "\n",
    "print(len(train_data), len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "\ttransforms.Resize((80, 300)), # height, width\n",
    "\ttransforms.Grayscale(),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.5), (0.5)),\n",
    "])\n",
    "\n",
    "def encode_formula(formula: str) -> list[int]:\n",
    "\tpadding = token2enc[\"<P>\"]\n",
    "\n",
    "\treturn [\n",
    "\t\tint(token2enc.get(t, padding)) for t in formula.split()\n",
    "\t]\n",
    "\n",
    "def decode_label(label: torch.Tensor) -> list[str]:\n",
    "\treturn [\n",
    "\t\tenc2token.get(str(c)) for c in label.tolist()\n",
    "\t]\n",
    "\n",
    "\n",
    "class LaTexDataset(Dataset):\n",
    "\tdef __init__(self, img2labels: dict, token2enc: dict, transform: transforms.Compose):\n",
    "\t\tself.transform = transform\n",
    "\t\t\n",
    "\t\tself.images = list(img2labels.keys())\n",
    "\t\tself.formulas = list(img2labels.values())\n",
    "\t\t\n",
    "\t\tself.token2enc = token2enc\n",
    "\n",
    "\t\t# const\n",
    "\t\tself.root_dir = \"./archive/PRINTED_TEX_230k/generated_png_images/\"\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timg_name = self.images[idx]\n",
    "\t\tformula = self.formulas[idx]\n",
    "\t\tlabel = torch.tensor(encode_formula(formula))\n",
    "\t\t\n",
    "\t\t# get img and apply transform\n",
    "\t\timg = Image.open(f'{self.root_dir}{img_name}')\n",
    "\t\timg = self.transform(img)\n",
    "\t\t\n",
    "\t\treturn img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    img, label = zip(*batch)\n",
    "    \n",
    "    img = torch.stack(img)\n",
    "    label = pad_sequence(label, batch_first=True, padding_value=2)\n",
    "    label = label.long()\n",
    "    \n",
    "    return img.to(device), label.to(device)\n",
    "\n",
    "train_dataset = LaTexDataset(train_data, token2enc, transform=trans)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataset = LaTexDataset(valid_data, token2enc, transform=trans)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 300])\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]) tensor([ 57, 543, 575,  21,  24, 577,  50, 543, 575,  21, 577,  57, 543, 575,\n",
      "         24,  21, 577, 549,  50, 543, 575,  24, 577,  36, 549,  50, 543, 575,\n",
      "         24, 577,  57, 543, 575,  21,  24, 577,  50, 543, 575,  21, 577,  57,\n",
      "        543, 575,  21,  24, 577, 542, 575,  11,  21, 577,  10])\n",
      "R _ { 1 2 } K _ { 1 } R _ { 2 1 } d K _ { 2 } = d K _ { 2 } R _ { 1 2 } K _ { 1 } R _ { 1 2 } ^ { - 1 } ,\n"
     ]
    }
   ],
   "source": [
    "print(img.size())\n",
    "print(img, label)\n",
    "\n",
    "print(' '.join(decode_label(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "\tdef __init__(self, feature_dim) -> None:\n",
    "\t\tsuper(EncoderCNN, self).__init__()\n",
    "\n",
    "\t\t# input size: (1, 80, 300)\n",
    "\n",
    "\t\tself.conv1 = nn.Conv2d(1, 64, 5)  # (64, 76, 296)\n",
    "\t\tself.pool = nn.MaxPool2d(2)  # (64, 38, 148)\n",
    "\t\tself.conv2 = nn.Conv2d(64, 128, 5)  # (128, 34, 144)\n",
    "\t\t# max pool -> (128, 17, 72)\n",
    "\t\tself.conv3 = nn.Conv2d(128, 256, 5)  # (256, 13, 68)\n",
    "\t\t# max pool -> (256, 6, 34)\n",
    "\n",
    "\t\t# flatten -> (256 * 6 * 34)\n",
    "\t\tself.dense1 = nn.Linear(256 * 6 * 34, feature_dim * 2)\n",
    "\t\tself.dense2 = nn.Linear(feature_dim * 2, feature_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "\t\tx = x.view(x.size(0), -1)\n",
    "\t\tx = F.relu(self.dense1(x))\n",
    "\t\tx = self.dense2(x)\n",
    "\t\t\n",
    "\t\treturn x\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "\tdef __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "\t\tsuper(DecoderRNN, self).__init__()\n",
    "\t\tself.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\t\tself.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "\tdef forward(self, features, formulas):\n",
    "\t\t# Embed the input formula tokens\n",
    "\t\tembeddings = self.embedding(formulas)\n",
    "\n",
    "\t\t# Concatenate features and embeddings along the sequence dimension\n",
    "\t\tembeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "\t\t\n",
    "\t\t# Pass through LSTM and then through the final linear layer\n",
    "\t\tlstm_out, _ = self.lstm(embeddings)\n",
    "\t\toutputs = self.fc(lstm_out)\n",
    "\t\treturn outputs\n",
    "\n",
    "class ImageToLaTeXModel(nn.Module):\n",
    "\tdef __init__(self, encoder, decoder):\n",
    "\t\tsuper(ImageToLaTeXModel, self).__init__()\n",
    "\t\tself.encoder = encoder\n",
    "\t\tself.decoder = decoder\n",
    "\n",
    "\tdef forward(self, images, formulas):\n",
    "\t\t# Encode the images\n",
    "\t\tfeatures = self.encoder(images)  # Shape: [batch_size, feature_dim]\n",
    "\t\t\n",
    "\t\t# Decode to generate the LaTeX expression\n",
    "\t\toutputs = self.decoder(features, formulas[:, :-1])\n",
    "\t\treturn outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Loss: 3.1988\n",
      "epoch 1\n",
      "Loss: 2.8262\n",
      "epoch 2\n",
      "Loss: 2.6768\n",
      "epoch 3\n",
      "Loss: 2.5749\n",
      "epoch 4\n",
      "Loss: 2.4905\n",
      "epoch 5\n",
      "Loss: 2.4183\n",
      "epoch 6\n",
      "Loss: 2.3525\n",
      "epoch 7\n",
      "Loss: 2.2906\n",
      "epoch 8\n",
      "Loss: 2.2283\n",
      "epoch 9\n",
      "Loss: 2.1668\n",
      "epoch 10\n",
      "Loss: 2.1071\n",
      "epoch 11\n",
      "Loss: 2.0506\n",
      "epoch 12\n",
      "Loss: 1.9911\n",
      "epoch 13\n",
      "Loss: 1.9346\n",
      "epoch 14\n",
      "Loss: 1.8786\n",
      "epoch 15\n",
      "Loss: 1.8189\n",
      "epoch 16\n",
      "Loss: 1.7607\n",
      "epoch 17\n",
      "Loss: 1.7080\n",
      "epoch 18\n",
      "Loss: 1.6600\n",
      "epoch 19\n",
      "Loss: 1.6077\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "model = ImageToLaTeXModel(encoder, decoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=2)  # 2 is assumed as <P> token\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\tprint(f\"epoch {epoch}\")\n",
    "\n",
    "\trunning_loss = 0.0\n",
    "\t\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\timgs, labels = data\n",
    "\n",
    "\t\t# Set targets: shift formula by one for teacher forcing\n",
    "\t\ttargets = labels[:, 1:]\n",
    "\n",
    "\t\t# Forward, loss, and optimize\n",
    "\t\toutputs = model(imgs, labels[:, :-1])\n",
    "\t\tloss = criterion(outputs.view(-1, vocab_size), targets.contiguous().view(-1))\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss += loss.item()\n",
    "\n",
    "\tprint(f\"Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Loss: 1.5566\n",
      "epoch 1\n",
      "Loss: 1.5062\n",
      "epoch 2\n",
      "Loss: 1.4668\n",
      "epoch 3\n",
      "Loss: 1.4355\n",
      "epoch 4\n",
      "Loss: 1.3914\n",
      "epoch 5\n",
      "Loss: 1.3437\n",
      "epoch 6\n",
      "Loss: 1.3102\n",
      "epoch 7\n",
      "Loss: 1.2735\n",
      "epoch 8\n",
      "Loss: 1.2365\n",
      "epoch 9\n",
      "Loss: 1.2100\n",
      "epoch 10\n",
      "Loss: 1.1940\n",
      "epoch 11\n",
      "Loss: 1.1534\n",
      "epoch 12\n",
      "Loss: 1.1230\n",
      "epoch 13\n",
      "Loss: 1.0969\n",
      "epoch 14\n",
      "Loss: 1.0766\n",
      "epoch 15\n",
      "Loss: 1.0691\n",
      "epoch 16\n",
      "Loss: 1.0299\n",
      "epoch 17\n",
      "Loss: 1.0134\n",
      "epoch 18\n",
      "Loss: 1.0024\n",
      "epoch 19\n",
      "Loss: 0.9787\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\tprint(f\"epoch {epoch}\")\n",
    "\n",
    "\trunning_loss = 0.0\n",
    "\t\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\timgs, labels = data\n",
    "\n",
    "\t\t# Set targets: shift formula by one for teacher forcing\n",
    "\t\ttargets = labels[:, 1:]\n",
    "\n",
    "\t\t# Forward, loss, and optimize\n",
    "\t\toutputs = model(imgs, labels[:, :-1])\n",
    "\t\tloss = criterion(outputs.view(-1, vocab_size), targets.contiguous().view(-1))\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\trunning_loss += loss.item()\n",
    "\n",
    "\tprint(f\"Loss: {running_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \\frac { 4 \\pi ^ { 2 } } { N g _ { Y M } ^ { 2 } } = F ( \\rho ) <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P> <P>\n",
      "P: _ 1 } \\rho _ 2 } } { M ^ { { c } } ^ { 2 } } = \\frac { 1 ) = \\frac { { { { { { { { { { { { { { { { { { { { { { { { { { { { . . . . . . . . . \\Gamma \\Gamma A A A A A A A A A A A A \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\kappa \\quad \\quad \\quad \\kappa \\kappa \\kappa \\kappa \\right) \\right) \\right) \\kappa { { { { { { { { \\kappa \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\delta \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) \\right) ) ) \\right) \\right) { { { { { { { { {\n",
      "\n",
      "0.05013327663009231 0.09109375\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "total_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "\tfor images, formulas in valid_loader:\n",
    "\t\timages, formulas = images.to(device), formulas.to(device)\n",
    "\t\toutputs = model(images, formulas[:, :-1])  # Pass images and input sequence\n",
    "\n",
    "\t\t# Calculate loss\n",
    "\t\tloss = criterion(outputs.view(-1, outputs.size(-1)), formulas[:, 1:].contiguous().view(-1))\n",
    "\t\ttotal_loss += loss.item()\n",
    "\n",
    "\t\t# Calculate accuracy (if applicable)\n",
    "\t\tpredicted_indices = torch.argmax(outputs, dim=2)  # Get the index of the max log-probability\n",
    "\t\tcorrect_predictions += (predicted_indices == formulas[:, 1:].contiguous()).sum().item()\n",
    "\t\ttotal_samples += formulas[:, 1:].numel()  # Total number of tokens in the validation batch\n",
    "\n",
    "\t\t# Print images and predictions\n",
    "\t\tfor i in range(len(images)):\n",
    "\t\t\t# Decode the actual and predicted formulas\n",
    "\n",
    "\t\t\tactual_formula = decode_label(formulas[i, :])  # Skip <S> token\n",
    "\t\t\tpredicted_formula = decode_label(predicted_indices[i, :])\n",
    "\n",
    "\t\t\tprint(f'A: {' '.join(actual_formula)}')\n",
    "\t\t\tprint(f'P: {' '.join(predicted_formula)}')\n",
    "\t\t\tprint()\n",
    "\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\tbreak\n",
    "\n",
    "avg_loss = total_loss / len(valid_loader)\n",
    "accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "print(avg_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
