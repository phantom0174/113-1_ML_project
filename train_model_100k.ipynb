{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Ti\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(device)\n",
    "# print(len(os.listdir('LaTex_data/split_1')))\n",
    "# print(image_formula_mapping['0002475406d9932.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # do not resize\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "label_to_index_file = './230k.json'\n",
    "with open(label_to_index_file, 'r') as f:\n",
    "\tsign2id = json.load(f)\n",
    "\n",
    "id2sign = [0] * 650\n",
    "for k, v in sign2id.items():\n",
    "\tid2sign[int(v)] = k\n",
    "\n",
    "def collate_fn(batch):\n",
    "\t# filter the pictures that have different weight or height\n",
    "\tsize = batch[0][0].size()\n",
    "\tbatch = [img_formula for img_formula in batch\n",
    "\t\t\tif img_formula[0].size() == size]\n",
    "\t\n",
    "\t# # sort by the length of formula\n",
    "\t# batch.sort(key=lambda img_formula: len(img_formula[1].split()),\n",
    "\t# \t\treverse=True)\n",
    "\n",
    "\timgs, formulas = zip(*batch)\n",
    "\tformulas = pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "\t\n",
    "\timgs = torch.stack(imgs, dim=0)\n",
    "\treturn imgs.to(device), formulas.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join\n",
    "\n",
    "class Im2LatexDataset(Dataset):\n",
    "\tdef __init__(self, data_dir, split, max_len=32 * 750):\n",
    "\t\t\"\"\"args:\n",
    "\t\tdata_dir: root dir storing the prepoccessed data\n",
    "\t\tsplit: train, validate or test\n",
    "\t\t\"\"\"\n",
    "\t\tassert split in [\"train\", \"validate\", \"test\"]\n",
    "\t\tself.data_dir = data_dir\n",
    "\t\tself.split = split\n",
    "\t\tself.max_len = max_len\n",
    "\t\tself.pairs = self._load_pairs()\n",
    "\n",
    "\tdef _load_pairs(self):\n",
    "\t\tpairs = torch.load(join(self.data_dir, \"{}.pkl\".format(self.split)))\n",
    "\n",
    "\t\tfinite_pairs = []\n",
    "\t\tfor i, (img, formula) in enumerate(pairs):\n",
    "\t\t\tpair = (img, \" \".join(formula.split()))\n",
    "\t\t\tfinite_pairs.append(pair)\n",
    "\n",
    "\t\t\tif i >= self.max_len:\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\treturn finite_pairs\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage, formula = self.pairs[idx]\n",
    "\t\t\n",
    "\t\tformula_tokens = '<S> ' + formula + ' <E> '\n",
    "\t\tformula_tokens = formula.split()\n",
    "\t\t\n",
    "\t\tformula_indices = []\n",
    "\t\tfor token in formula_tokens:\n",
    "\t\t\t# Map each token to its index; if not found, use a default index (e.g., 0)\n",
    "\t\t\tindex = sign2id.get(token, 0)  # Assuming 0 is for unknown tokens\n",
    "\t\t\tformula_indices.append(int(index))\n",
    "\t\t\n",
    "\t\treturn image, torch.tensor(formula_indices, dtype=torch.long)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\tIm2LatexDataset('./100k/', 'train'),\n",
    "\tbatch_size=batch_size,\n",
    "\tcollate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\tIm2LatexDataset('./100k/', 'validate'),\n",
    "    batch_size=32,\n",
    "\tcollate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          ...,\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816]],\n",
      "\n",
      "         [[0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          ...,\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911]],\n",
      "\n",
      "         [[0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          ...,\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352]]],\n",
      "\n",
      "\n",
      "        [[[0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          ...,\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385]],\n",
      "\n",
      "         [[0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          ...,\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478]],\n",
      "\n",
      "         [[0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          ...,\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819]]],\n",
      "\n",
      "\n",
      "        [[[0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          ...,\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088]],\n",
      "\n",
      "         [[0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          ...,\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678]],\n",
      "\n",
      "         [[0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          ...,\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          ...,\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603]],\n",
      "\n",
      "         [[0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          ...,\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359]],\n",
      "\n",
      "         [[0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          ...,\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          ...,\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231]],\n",
      "\n",
      "         [[0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          ...,\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233]],\n",
      "\n",
      "         [[0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          ...,\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695]]]],\n",
      "       device='cuda:0'), tensor([[572, 543, 575,  ...,   2,   2,   2],\n",
      "        [287,  43, 543,  ...,   2,   2,   2],\n",
      "        [ 83, 543, 575,  ..., 557, 577,  71],\n",
      "        ...,\n",
      "        [561, 542, 575,  ...,   2,   2,   2],\n",
      "        [575, 166,  51,  ...,   2,   2,   2],\n",
      "        [  6, 382, 563,  ...,   2,   2,   2]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    img, label = data\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder / Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_out_dim, dec_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(enc_out_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        B, seq_len, enc_dim = encoder_outputs.shape\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, hidden), dim=2)))\n",
    "        energy = energy @ (self.v / torch.sqrt(torch.tensor(enc_dim, dtype=torch.float)))\n",
    "        attn_weights = F.softmax(energy, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, enc_out_dim=512, dropout_prob=0):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "\n",
    "\n",
    "            nn.Conv2d(256, enc_out_dim, 3, 1, 0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn_encoder(images)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        B, H, W, C = features.shape\n",
    "        features = features.contiguous().view(B, H * W, C)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, enc_out_dim, dropout_prob=0.3):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = Attention(enc_out_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(enc_out_dim + embedding_dim, hidden_dim, batch_first=True)  # LSTM only takes features\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, features, formulas, teacher_forcing_ratio=0.7):\n",
    "        batch_size = features.size(0)\n",
    "        seq_len = formulas.size(1)\n",
    "        vocab_size = self.fc.out_features\n",
    "\n",
    "        hidden = torch.zeros(1, batch_size, self.lstm.hidden_size, device=features.device)\n",
    "        cell = torch.zeros(1, batch_size, self.lstm.hidden_size, device=features.device)\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, seq_len, vocab_size, device=features.device)\n",
    "\n",
    "        input_token = torch.ones((batch_size, 1), dtype=torch.long, device=features.device) * 0  # Shape: (batch_size, 1)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            input_emb = self.embedding(input_token)  # Shape: (batch_size, 1, embedding_dim)\n",
    "            \n",
    "            context, _ = self.attention(features, hidden.squeeze(0))  # Shape: (batch_size, enc_out_dim)\n",
    "            context = context.unsqueeze(1)  # Add time dimension: (batch_size, 1, enc_out_dim)\n",
    "            \n",
    "            lstm_input = torch.cat((context, input_emb), dim=2)  # Shape: (batch_size, 1, enc_out_dim + embedding_dim)\n",
    "            \n",
    "            lstm_out, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))  # Shape: (batch_size, 1, hidden_dim)\n",
    "            \n",
    "            output = self.fc(lstm_out.squeeze(1))  # Shape: (batch_size, vocab_size)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            top1 = output.argmax(1)  # Shape: (batch_size)\n",
    "            if self.training:\n",
    "                # Decide whether to use teacher forcing\n",
    "                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "                input_token = formulas[:, t].unsqueeze(1) if teacher_force and t + 1 < seq_len else top1.unsqueeze(1)\n",
    "            else:\n",
    "                input_token = top1.unsqueeze(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ImageToLaTeXModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageToLaTeXModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, formulas):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, formulas[:, :-1])\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_state(model, optimizer, epoch, loss):\n",
    "\tstate = {\n",
    "\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t'epoch': epoch,\n",
    "\t\t'loss': loss\n",
    "\t}\n",
    "\ttorch.save(state, 'model_checkpoint.pth')\n",
    "\n",
    "# Function to load model state\n",
    "def load_training_state(model, optimizer):\n",
    "\tcheckpoint = torch.load('model_checkpoint.pth')\n",
    "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
    "\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\treturn checkpoint['epoch'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get model from epoch 10, with loss 0.3491\n",
      "Epoch [11/22], Step [0/751], Loss: 1.6123\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m\n\u001b[0;32m     24\u001b[0m \tstart_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n\u001b[1;32m---> 29\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformulas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[38;5;66;43;03m# Pad sequences to the same length\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\bibby\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\bibby\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\bibby\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 26\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     23\u001b[0m formulas \u001b[38;5;241m=\u001b[39m pad_sequence(formulas, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     25\u001b[0m imgs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(imgs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, formulas\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBED_SIZE = 512 # direct output dim from cv_tiny\n",
    "\n",
    "hidden_size = 1024\n",
    "num_epochs = 22\n",
    "learning_rate = 0.003\n",
    "\n",
    "vocab_size = len(sign2id)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "encoder = EncoderCNN(EMBED_SIZE).to(device)\n",
    "decoder = DecoderRNN(EMBED_SIZE, hidden_size, vocab_size,512).to(device)\n",
    "model = ImageToLaTeXModel(encoder, decoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "start_epoch = 0\n",
    "\n",
    "# Try to resume from a checkpoint\n",
    "try:\n",
    "\tstart_epoch, last_loss = load_training_state(model, optimizer)\n",
    "\tprint(f\"Get model from epoch {start_epoch}, with loss {last_loss:.4f}\")\n",
    "except FileNotFoundError:\n",
    "\tprint(\"No saved model found, starting fresh.\")\n",
    "\tstart_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "\t\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\timages, formulas = data\n",
    "\t\t# Pad sequences to the same length\n",
    "\t\tformulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "\t\ttargets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "\t\toutputs = model(images, formulas_padded[:, :-1].contiguous())\n",
    "\t\t# Match target size with output size\n",
    "\t\ttargets = targets[:, :outputs.size(1)].contiguous()\n",
    "\n",
    "\t\tloss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif i  % 100 == 0:\n",
    "\t\t\tprint(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\tsave_training_state(model, optimizer, epoch, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfsadf asdlkfj.ds .df.daf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 42, 579])\n",
      "torch.Size([32, 42])\n",
      "act : _ { 1 } ^ { k } = \\omega _ { 1 } ^ { k - 2 } \\subseteq \\omega _ { 1 } ^ { k }\n",
      "pred: _ { \\mu } ^ { \\mu } = 0 _ { k } ^ { j } 1 } } { _ { k } ^ { k } } _\n",
      "\n",
      "act : _ { i j } = \\bar { g } _ { i j } + h _ { i j } ,\n",
      "pred: _ { \\mu j } = { { g } _ { i j } ^ g _ { i j } g\n",
      "\n",
      "act : \\theta _ { n } ^ { \\alpha \\Lambda } } { ^ \\dagger } = \\theta _ { - n } ^ { \\alpha \\Lambda } ,\n",
      "pred: _ ^ { \\alpha } , { i } } = 0 , = 0 { \\alpha } } ^ { \\alpha } } {\n",
      "\n",
      "act : _ { \\mathrm { S G } } = 2 g _ { \\mathrm { e f f } } .\n",
      "pred: _ { \\mu { e } } } = { } = 0 . 1 } ^ } } =\n",
      "\n",
      "act : \\chi _ { D } ^ { \\prime \\prime } , \\chi _ { D } ^ { \\prime \\prime } \\} = i \\chi ^ { \\prime \\prime } .\n",
      "pred: Q _ { \\alpha } , { i } } , \\chi _ { \\beta } ^ { \\prime } } \\} = 0 \\delta ^ { \\prime } } ,\n",
      "\n",
      "act : ^ { 2 } f = \\bar { \\gamma } F ^ { 4 } f \\; \\;\n",
      "pred: _ { 2 } { ^ 0 { f } ^ { 2 } \\, \\, ^ { \\;\n",
      "\n",
      "act : _ { m n p } \\, \\xi = \\psi _ { m n p } \\, \\xi ,\n",
      "pred: _ { \\mu } ^ _ = = _ 0 _ { m } ^ _ \\, , _\n",
      "\n",
      "act : = d \\alpha + \\alpha ^ { 2 } ,\n",
      "pred: _ \\theta _ ^ { ^ { \\prime } \\theta\n",
      "\n",
      "act : \\left( t \\right) \\stackrel { t \\rightarrow \\infty } { { \\rightarrow } } 1 / 2\n",
      "pred: _ \\phi \\right) = 0 \\cal } \\infty } \\left( \\longrightarrow } \\left( { \\right) } 2 }\n",
      "\n",
      "act : _ { \\rho } \\dot { Z } ^ { 1 } = \\partial _ { \\sigma } \\dot { Z } ^ { 2 } ,\n",
      "pred: _ { \\mu } F { x } ^ { 2 } = 0 . { 0 } ^ { X } ^ { 2 } =\n",
      "\n",
      "act : X , Y ] = - i \\ell _ { B } ^ { 2 } .\n",
      "pred: a ^ { ] _ 0 [ [ [ { 0 } [ X , Y , Y\n",
      "\n",
      "act : P _ { l } , J ] = - i \\epsilon _ { l n } P _ { n } ,\n",
      "pred: \\hat _ { i } , P _ = 0 i \\epsilon _ { i } } , _ { i } ^\n",
      "\n",
      "act : ^ { i } \\ \\longrightarrow \\ \\theta ^ { i } + \\epsilon \\xi ^ { i } \\ ,\n",
      "pred: _ { \\alpha j } = } \\theta ^ { i } \\ , \\in { j } \\ ,\n",
      "\n",
      "act : =\n",
      "pred: _ \\Gamma ^\n",
      "\n",
      "act : _ { i } = m K _ { i } ^ { n } v _ { n c } \\sigma ^ { c } .\n",
      "pred: _ { \\mathrm } ^ { ^ _ { i } ^ { 2 } + _ { i } } ^ _ { i } +\n",
      "\n",
      "act : { \\phi } , \\: \\tilde { \\psi } , \\: \\tilde { H }\n",
      "pred: { \\psi } _ { \\: { \\psi } _ =\n",
      "\n",
      "act : \\delta A _ { \\mu } ~ = ~ \\partial _ { \\mu } \\alpha\n",
      "pred: ^ _ { { \\mu } } , ~ \\delta _ { \\mu } \\epsilon _\n",
      "\n",
      "act : \\partial _ { k _ { 1 } k _ { 2 } } h _ { i j } ) I _ { h } ^ { i j ( k _ { 1 } k _ { 2 } ) }\n",
      "pred: _ _ { \\mu } { 1 } } _ { 2 } } ) _ { k _ } } = _ { i _ { { i j } } _ { i } } { j }\n",
      "\n",
      "act : W \\gamma _ { 0 } ) ^ { \\dagger } = W \\gamma _ { 0 } ,\n",
      "pred: _ ^ { { \\mu } ) ^ { 2 } = 0 { 0 } ^\n",
      "\n",
      "act : { \\phi } \\rightarrow U \\hat { \\phi } U ^ { \\dagger } .\n",
      "pred: { H } _ { ^ { U } = ^ { - 1 }\n",
      "\n",
      "act : \\equiv \\chi ^ { - 1 } ~ \\phi ~ \\chi , \\;\n",
      "pred: _ e ^ { - i } { , ~ ,\n",
      "\n",
      "act : ^ { 9 } \\times S _ { R ^ { 1 1 } } ^ { S S } \\times S _ { R } ^ { 1 }\n",
      "pred: _ { 2 } = 1 ^ { 9 } { 9 } } = = { 2 } } = S ^ { S S ^ { S } = 0\n",
      "\n",
      "act : _ { k } ^ { - p } = ( A _ { k } ^ { p } ) ^ { \\dagger } ;\n",
      "pred: _ { \\mu } ^ { ( 1 } = A _ { k } } ^ { k } } } { k } =\n",
      "\n",
      "act : u ^ { \\alpha } , u ^ { \\beta } ] = i \\Theta ^ { \\alpha \\beta } ,\n",
      "pred: a , v ] = ] { ) { * } ] = 0 , _ { \\alpha \\beta } ,\n",
      "\n",
      "act : | \\chi \\rangle = Q | \\Lambda \\rangle\n",
      "pred: _ _ { = 0 ^ ^ \\rangle =\n",
      "\n",
      "act : _ { k } \\approx e ^ { - i \\omega t }\n",
      "pred: _ { i } ^ 0 ^ { - 2 k _ { 0 }\n",
      "\n",
      "act : \\cal B } _ { 1 } = { \\cal B } _ { 2 } \\rightarrow 0 \\ .\n",
      "pred: _ L } _ { \\mathrm } = { \\cal { } _ { 2 } } = , ,\n",
      "\n",
      "act : L ^ { \\pm } = L ^ { \\pm } \\dot { \\otimes } L ^ { \\pm }\n",
      "pred: { _ { i } = { _ { i } } L ^ { = ^ { \\pm } ,\n",
      "\n",
      "act : _ { { \\bf q } } = \\theta ( q ^ { r } ) \\, n ( q ^ { r } ) ,\n",
      "pred: _ { i \\mu } } ^ ^ { _ n ) { \\mu } , _ { } q ) { 2 } ) ,\n",
      "\n",
      "act : u \\gg { \\hat { g } } ^ { 1 / 2 } \\gg 1 .\n",
      "pred: _ = a \\bf u } } } { { - 1 } , 1 ,\n",
      "\n",
      "act : \\mathcal { S } _ { C S } ( A ) = 0 \\; ,\n",
      "pred: _ { A } = { \\mathrm S } = { ) = { \\mathcal \\;\n",
      "\n",
      "act : ^ { - \\tilde { q } } h ^ { - 1 } 2 ( - \\tilde { q } ) \\upsilon ^ { 2 }\n",
      "pred: _ { 2 1 } 1 } } = _ { - 1 } = } 1 + { h } ^ ^ ^ { - } }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_formula(indices, id2sign):\n",
    "    return ' '.join([id2sign[i.item()] for i in indices if i.item() and i.item() != 2])  # Skip padding\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader):\n",
    "        images, formulas = data\n",
    "        # Pad sequences to the same length\n",
    "        formulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "        targets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "        outputs = model(images, formulas_padded[:, :-1].contiguous())\n",
    "        print(outputs.shape)\n",
    "\n",
    "        predicted_indices = torch.argmax(outputs, dim=2)\n",
    "        print(predicted_indices.shape)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            actual_formula = decode_formula(formulas[i, 1:], id2sign)  # Skip <S> token\n",
    "            predicted_formula = decode_formula(predicted_indices[i, 1:], id2sign)  # Skip <S> token\n",
    "\n",
    "            print('act :', actual_formula)\n",
    "            print('pred:', predicted_formula)\n",
    "            print()\n",
    "        break\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 108\u001b[0m\n\u001b[0;32m    104\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m total_bleu \u001b[38;5;241m/\u001b[39m total_imgs\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, accuracy\n\u001b[1;32m--> 108\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% , val_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 76\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, criterion, device)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, formulas \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     75\u001b[0m     images, formulas \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), formulas\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_formula\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     77\u001b[0m     formulas_padded \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(formulas, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     78\u001b[0m     targets \u001b[38;5;241m=\u001b[39m formulas_padded[:, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m, in \u001b[0;36mpredict_formula\u001b[1;34m(model, image, start_token, end_token, max_length, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Extract features using the encoder\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Initialize the decoder input with the <START> token\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     input_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[start_token]], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\bibby\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m, in \u001b[0;36mEncoderCNN.forward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[0;32m     49\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_encoder(images)\n\u001b[1;32m---> 50\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     B, H, W, C \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     52\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, H \u001b[38;5;241m*\u001b[39m W, C)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def predict_formula(model, image, start_token, end_token, max_length, device):\n",
    "    \"\"\"\n",
    "    Generate a LaTeX formula given an image input.\n",
    "\n",
    "    Args:\n",
    "        model: The trained ImageToLaTeXModel.\n",
    "        image: Input image tensor of shape (1, C, H, W).\n",
    "        start_token: Index of the <START> token.\n",
    "        end_token: Index of the <END> token.\n",
    "        max_length: Maximum length of the generated formula.\n",
    "        device: Device to perform inference on.\n",
    "\n",
    "    Returns:\n",
    "        A list of token indices representing the generated formula.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract features using the encoder\n",
    "        features = model.encoder(image)\n",
    "\n",
    "        # Initialize the decoder input with the <START> token\n",
    "        input_token = torch.tensor([[start_token]], device=device)\n",
    "\n",
    "        # Initialize LSTM hidden and cell states\n",
    "        hidden = torch.zeros(1, 1, model.decoder.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, 1, model.decoder.lstm.hidden_size, device=device)\n",
    "\n",
    "        # Store generated tokens\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Embed the current token\n",
    "            embedding = model.decoder.embedding(input_token).squeeze(1)\n",
    "\n",
    "            # Compute attention and context\n",
    "            context, _ = model.decoder.attention(features, hidden.squeeze(0))\n",
    "\n",
    "            # Prepare input for the LSTM\n",
    "            lstm_input = torch.cat((embedding, context), dim=1).unsqueeze(1)\n",
    "\n",
    "            # Forward through LSTM\n",
    "            lstm_out, (hidden, cell) = model.decoder.lstm(lstm_input, (hidden, cell))\n",
    "\n",
    "            # Generate the next token\n",
    "            output = model.decoder.fc(lstm_out.squeeze(1))\n",
    "            next_token = output.argmax(dim=1)\n",
    "\n",
    "            # Append the predicted token\n",
    "            generated_tokens.append(next_token.item())\n",
    "\n",
    "            # Break if <END> token is generated\n",
    "            if next_token.item() == end_token:\n",
    "                break\n",
    "\n",
    "            # Update input token for the next time step\n",
    "            input_token = next_token.unsqueeze(1)\n",
    "\n",
    "    return generated_tokens\n",
    "\n",
    "def decode_formula(indices, id2sign):\n",
    "    return ' '.join([id2sign[i.item()] for i in indices if i.item() and i.item() != 2])  # Skip padding\n",
    "\n",
    "def validate_model(model, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_bleu = 0\n",
    "    total_samples = 0\n",
    "    total_imgs = 0\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, formulas in val_loader:\n",
    "            images, formulas = images.to(device), formulas.to(device)\n",
    "            print(predict_formula(model, images[0], 0, 2, 100, device))\n",
    "            formulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "            targets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "            outputs = model(images, [])\n",
    "            # Match target size with output size\n",
    "            targets = targets[:, :outputs.size(1)].contiguous()\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            predicted_indices = torch.argmax(outputs, dim=2)  # Get the index of the max log-probability\n",
    "            total_samples += formulas[:, 1:].numel()  # Total number of tokens in the validation batch\n",
    "\n",
    "            # Print images and predictions\n",
    "            # The batch size is 400, print every 400 images\n",
    "            total_imgs += len(images)\n",
    "            for i in range(len(images)):\n",
    "                # Decode the actual and predicted formulas\n",
    "                actual_formula = decode_formula(formulas[i, :], id2sign)  \n",
    "                predicted_formula = decode_formula(predicted_indices[i, :], id2sign) \n",
    "                total_bleu += sentence_bleu([actual_formula.split()], predicted_formula.split(), smoothing_function=smooth_fn)\n",
    "                # print(total_bleu)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if i == 0:\n",
    "                    print(f'Actual Formula:    {actual_formula}')\n",
    "                    print(f'Predicted Formula: {predicted_formula}')\n",
    "                    print('-' * 50)\n",
    "\n",
    "    avg_loss = total_loss / total_imgs\n",
    "    accuracy = total_bleu / total_imgs\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# val_loss, val_accuracy = validate_model(model, criterion, device)\n",
    "# print(f'val_accuracy: {val_accuracy*100:.2f}% , val_loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
