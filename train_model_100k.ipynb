{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Ti\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(device)\n",
    "# print(len(os.listdir('LaTex_data/split_1')))\n",
    "# print(image_formula_mapping['0002475406d9932.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # do not resize\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "label_to_index_file = './230k.json'\n",
    "with open(label_to_index_file, 'r') as f:\n",
    "\tsign2id = json.load(f)\n",
    "\n",
    "id2sign = [0] * 650\n",
    "for k, v in sign2id.items():\n",
    "\tid2sign[int(v)] = k\n",
    "\n",
    "def collate_fn(batch):\n",
    "\t# filter the pictures that have different weight or height\n",
    "\tsize = batch[0][0].size()\n",
    "\tbatch = [img_formula for img_formula in batch\n",
    "\t\t\tif img_formula[0].size() == size]\n",
    "\t\n",
    "\t# # sort by the length of formula\n",
    "\t# batch.sort(key=lambda img_formula: len(img_formula[1].split()),\n",
    "\t# \t\treverse=True)\n",
    "\n",
    "\timgs, formulas = zip(*batch)\n",
    "\tformulas = pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "\t\n",
    "\timgs = torch.stack(imgs, dim=0)\n",
    "\treturn imgs.to(device), formulas.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join\n",
    "\n",
    "class Im2LatexDataset(Dataset):\n",
    "\tdef __init__(self, data_dir, split, max_len=32 * 750):\n",
    "\t\t\"\"\"args:\n",
    "\t\tdata_dir: root dir storing the prepoccessed data\n",
    "\t\tsplit: train, validate or test\n",
    "\t\t\"\"\"\n",
    "\t\tassert split in [\"train\", \"validate\", \"test\"]\n",
    "\t\tself.data_dir = data_dir\n",
    "\t\tself.split = split\n",
    "\t\tself.max_len = max_len\n",
    "\t\tself.pairs = self._load_pairs()\n",
    "\n",
    "\tdef _load_pairs(self):\n",
    "\t\tpairs = torch.load(join(self.data_dir, \"{}.pkl\".format(self.split)))\n",
    "\n",
    "\t\tfinite_pairs = []\n",
    "\t\tfor i, (img, formula) in enumerate(pairs):\n",
    "\t\t\tpair = (img, \" \".join(formula.split()))\n",
    "\t\t\tfinite_pairs.append(pair)\n",
    "\n",
    "\t\t\tif i >= self.max_len:\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\treturn finite_pairs\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage, formula = self.pairs[idx]\n",
    "\t\t\n",
    "\t\tformula_tokens = '<S> ' + formula + ' <E> '\n",
    "\t\tformula_tokens = formula.split()\n",
    "\t\t\n",
    "\t\tformula_indices = []\n",
    "\t\tfor token in formula_tokens:\n",
    "\t\t\t# Map each token to its index; if not found, use a default index (e.g., 0)\n",
    "\t\t\tindex = sign2id.get(token, 0)  # Assuming 0 is for unknown tokens\n",
    "\t\t\tformula_indices.append(int(index))\n",
    "\t\t\n",
    "\t\treturn image, torch.tensor(formula_indices, dtype=torch.long)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\tIm2LatexDataset('./100k/', 'train'),\n",
    "\tbatch_size=batch_size,\n",
    "\tcollate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\tIm2LatexDataset('./100k/', 'validate'),\n",
    "    batch_size=32,\n",
    "\tcollate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          ...,\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816]],\n",
      "\n",
      "         [[0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          ...,\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911]],\n",
      "\n",
      "         [[0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          ...,\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352]]],\n",
      "\n",
      "\n",
      "        [[[0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          ...,\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385]],\n",
      "\n",
      "         [[0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          ...,\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478]],\n",
      "\n",
      "         [[0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          ...,\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819]]],\n",
      "\n",
      "\n",
      "        [[[0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          ...,\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088]],\n",
      "\n",
      "         [[0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          ...,\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678]],\n",
      "\n",
      "         [[0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          ...,\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          ...,\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603]],\n",
      "\n",
      "         [[0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          ...,\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359]],\n",
      "\n",
      "         [[0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          ...,\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          ...,\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231]],\n",
      "\n",
      "         [[0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          ...,\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233]],\n",
      "\n",
      "         [[0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          ...,\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695]]]],\n",
      "       device='cuda:0'), tensor([[572, 543, 575,  ...,   2,   2,   2],\n",
      "        [287,  43, 543,  ...,   2,   2,   2],\n",
      "        [ 83, 543, 575,  ..., 557, 577,  71],\n",
      "        ...,\n",
      "        [561, 542, 575,  ...,   2,   2,   2],\n",
      "        [575, 166,  51,  ...,   2,   2,   2],\n",
      "        [  6, 382, 563,  ...,   2,   2,   2]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    img, label = data\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder / Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_out_dim, dec_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(enc_out_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        B, seq_len, enc_dim = encoder_outputs.shape\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, hidden), dim=2)))\n",
    "        energy = energy @ (self.v / torch.sqrt(torch.tensor(enc_dim, dtype=torch.float)))\n",
    "        attn_weights = F.softmax(energy, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, enc_out_dim=512, dropout_prob=0):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "\n",
    "            nn.Conv2d(256, enc_out_dim, 3, 1, 0),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn_encoder(images)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        B, H, W, C = features.shape\n",
    "        features = features.contiguous().view(B, H * W, C)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, enc_out_dim, dropout_prob=0.3):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = Attention(enc_out_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(enc_out_dim + embedding_dim, hidden_dim, batch_first=True)  # LSTM only takes features\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, features, formulas, teacher_forcing_ratio=0.5):\n",
    "        batch_size = features.size(0)\n",
    "        seq_len = formulas.size(1)\n",
    "        vocab_size = self.fc.out_features\n",
    "\n",
    "        hidden = torch.zeros(1, batch_size, self.lstm.hidden_size, device=features.device)\n",
    "        cell = torch.zeros(1, batch_size, self.lstm.hidden_size, device=features.device)\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, seq_len, vocab_size, device=features.device)\n",
    "\n",
    "        input_token = torch.ones((batch_size, 1), dtype=torch.long, device=features.device) * 0  # Shape: (batch_size, 1)\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            input_emb = self.embedding(input_token)  # Shape: (batch_size, 1, embedding_dim)\n",
    "            \n",
    "            context, _ = self.attention(features, hidden.squeeze(0))  # Shape: (batch_size, enc_out_dim)\n",
    "            context = context.unsqueeze(1)  # Add time dimension: (batch_size, 1, enc_out_dim)\n",
    "            \n",
    "            lstm_input = torch.cat((context, input_emb), dim=2)  # Shape: (batch_size, 1, enc_out_dim + embedding_dim)\n",
    "            \n",
    "            lstm_out, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))  # Shape: (batch_size, 1, hidden_dim)\n",
    "            \n",
    "            output = self.fc(lstm_out.squeeze(1))  # Shape: (batch_size, vocab_size)\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            top1 = output.argmax(1)  # Shape: (batch_size)\n",
    "            if self.training:\n",
    "                # Decide whether to use teacher forcing\n",
    "                teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "                input_token = formulas[:, t].unsqueeze(1) if teacher_force and t + 1 < seq_len else top1.unsqueeze(1)\n",
    "            else:\n",
    "                input_token = top1.unsqueeze(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ImageToLaTeXModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageToLaTeXModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, formulas):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, formulas[:, :-1])\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_state(model, optimizer, epoch, loss):\n",
    "\tstate = {\n",
    "\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t'epoch': epoch,\n",
    "\t\t'loss': loss\n",
    "\t}\n",
    "\ttorch.save(state, 'model_checkpoint.pth')\n",
    "\n",
    "# Function to load model state\n",
    "def load_training_state(model, optimizer):\n",
    "\tcheckpoint = torch.load('model_checkpoint.pth')\n",
    "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
    "\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\treturn checkpoint['epoch'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get model from epoch 5, with loss 1.2459\n",
      "Epoch [6/50], Step [0/751], Loss: 0.7060\n",
      "Epoch [6/50], Step [100/751], Loss: 0.3105\n",
      "Epoch [6/50], Step [200/751], Loss: 0.3583\n",
      "Epoch [6/50], Step [300/751], Loss: 0.6593\n",
      "Epoch [6/50], Step [400/751], Loss: 0.6786\n",
      "Epoch [6/50], Step [500/751], Loss: 0.6671\n",
      "Epoch [6/50], Step [600/751], Loss: 0.7441\n",
      "Epoch [6/50], Step [700/751], Loss: 0.6930\n",
      "Epoch [7/50], Step [0/751], Loss: 0.6363\n",
      "Epoch [7/50], Step [100/751], Loss: 0.3039\n",
      "Epoch [7/50], Step [200/751], Loss: 0.5109\n",
      "Epoch [7/50], Step [300/751], Loss: 0.5742\n",
      "Epoch [7/50], Step [400/751], Loss: 0.4381\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBED_SIZE = 512 # direct output dim from cv_tiny\n",
    "\n",
    "hidden_size = 1024\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "vocab_size = len(sign2id)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "encoder = EncoderCNN(EMBED_SIZE).to(device)\n",
    "decoder = DecoderRNN(EMBED_SIZE, hidden_size, vocab_size,512).to(device)\n",
    "model = ImageToLaTeXModel(encoder, decoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "start_epoch = 0\n",
    "\n",
    "# Try to resume from a checkpoint\n",
    "try:\n",
    "\tstart_epoch, last_loss = load_training_state(model, optimizer)\n",
    "\tprint(f\"Get model from epoch {start_epoch}, with loss {last_loss:.4f}\")\n",
    "except FileNotFoundError:\n",
    "\tprint(\"No saved model found, starting fresh.\")\n",
    "\tstart_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "\t\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\timages, formulas = data\n",
    "\t\t# Pad sequences to the same length\n",
    "\t\ttargets = formulas[:, :].contiguous()\n",
    "\n",
    "\t\toutputs = model(images, formulas[:, :].contiguous())\n",
    "\t\t# Match target size with output size\n",
    "\t\ttargets = targets[:, :outputs.size(1)].contiguous()\n",
    "\n",
    "\t\tloss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif i  % 100 == 0:\n",
    "\t\t\tprint(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\tsave_training_state(model, optimizer, epoch, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfsadf asdlkfj.ds .df.daf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 42, 579])\n",
      "torch.Size([32, 42])\n",
      "act : _ { 1 } ^ { k } = \\omega _ { 1 } ^ { k - 2 } \\subseteq \\omega _ { 1 } ^ { k }\n",
      "pred: _ { 1 } ^ { k } = \\omega _ { 1 } ^ { k - 2 } \\, \\omega _ { 1 } ^ { k - 2 } \\, \\omega _ { 1 } ^ { k\n",
      "\n",
      "act : _ { i j } = \\bar { g } _ { i j } + h _ { i j } ,\n",
      "pred: _ { i j } = \\bar { g } _ { i j } + h _ { i j } + h _ { i j } + h _ { i j } ,\n",
      "\n",
      "act : \\theta _ { n } ^ { \\alpha \\Lambda } } { ^ \\dagger } = \\theta _ { - n } ^ { \\alpha \\Lambda } ,\n",
      "pred: _ { n } ^ { \\mathrm { A } } | } } = \\theta _ { - n } ^ { \\mathrm { A } } } } ^ { \\mathrm { A } } ,\n",
      "\n",
      "act : _ { \\mathrm { S G } } = 2 g _ { \\mathrm { e f f } } .\n",
      "pred: _ { \\mathrm { S S } } = 2 g _ { \\mathrm { e f f } } = 2 g _ { \\mathrm { e f f } } .\n",
      "\n",
      "act : \\chi _ { D } ^ { \\prime \\prime } , \\chi _ { D } ^ { \\prime \\prime } \\} = i \\chi ^ { \\prime \\prime } .\n",
      "pred: \\chi \\chi _ { D } ^ { \\prime } , \\chi _ { D } ^ { \\prime } , = i \\chi ^ { \\prime } , \\chi _ { D } ^ { \\prime } , = i\n",
      "\n",
      "act : ^ { 2 } f = \\bar { \\gamma } F ^ { 4 } f \\; \\;\n",
      "pred: ^ { 2 } f = \\bar { \\gamma } F ^ { 4 } f = \\bar { \\gamma } F ^ { 4 } f\n",
      "\n",
      "act : _ { m n p } \\, \\xi = \\psi _ { m n p } \\, \\xi ,\n",
      "pred: _ { m m p } \\, \\xi = \\psi _ { m m p } \\xi \\xi = = \\psi _ { m n p } \\xi \\xi ,\n",
      "\n",
      "act : = d \\alpha + \\alpha ^ { 2 } ,\n",
      "pred: = d \\alpha + \\alpha ^ { 2 } ,\n",
      "\n",
      "act : \\left( t \\right) \\stackrel { t \\rightarrow \\infty } { { \\rightarrow } } 1 / 2\n",
      "pred: \\left( t \\right) ^ { { - 2 } - 1 / l ^ { { } - 1 / l \\right) ^ { { - 2 } }\n",
      "\n",
      "act : _ { \\rho } \\dot { Z } ^ { 1 } = \\partial _ { \\sigma } \\dot { Z } ^ { 2 } ,\n",
      "pred: _ { \\rho } \\tilde { Z } ^ { 1 } = \\partial _ { \\sigma } \\tilde { Z } ^ { 1 } = \\partial _ { \\sigma } \\tilde { Z } ^ { 2 } ,\n",
      "\n",
      "act : X , Y ] = - i \\ell _ { B } ^ { 2 } .\n",
      "pred: X , Y ] = - i \\beta _ { D } ^ { 2 } .\n",
      "\n",
      "act : P _ { l } , J ] = - i \\epsilon _ { l n } P _ { n } ,\n",
      "pred: P _ { l } , J \\right] = - i \\epsilon _ { l m } P _ { m } P _ { n } ,\n",
      "\n",
      "act : ^ { i } \\ \\longrightarrow \\ \\theta ^ { i } + \\epsilon \\xi ^ { i } \\ ,\n",
      "pred: ^ { i } + \\rightarrow \\xi ^ { i } + \\epsilon ^ { i } + \\epsilon ^ { i } + \\epsilon ^ { i } + \\epsilon \\xi ^ { i } \\ ,\n",
      "\n",
      "act : =\n",
      "pred: = \\Gamma\n",
      "\n",
      "act : _ { i } = m K _ { i } ^ { n } v _ { n c } \\sigma ^ { c } .\n",
      "pred: _ { i } = m R _ { i } ^ { n } v ^ { n } v _ { n } ^ { n } v ^ { c } .\n",
      "\n",
      "act : { \\phi } , \\: \\tilde { \\psi } , \\: \\tilde { H }\n",
      "pred: { \\phi } , \\; \\tilde { { } } , \\; \\tilde { { } } , \\; \\tilde { { } } } \\; , \\tilde { { } }\n",
      "\n",
      "act : \\delta A _ { \\mu } ~ = ~ \\partial _ { \\mu } \\alpha\n",
      "pred: \\delta A _ { \\mu } ~ = ~ \\partial _ { \\mu } \\alpha A _ { \\mu } \\alpha\n",
      "\n",
      "act : \\partial _ { k _ { 1 } k _ { 2 } } h _ { i j } ) I _ { h } ^ { i j ( k _ { 1 } k _ { 2 } ) }\n",
      "pred: \\partial _ { k _ { 1 } } _ { i } { 1 } } ) _ { i } ^ { i i _ { 1 } } _ { i } { 1 } } _ {\n",
      "\n",
      "act : W \\gamma _ { 0 } ) ^ { \\dagger } = W \\gamma _ { 0 } ,\n",
      "pred: W \\gamma _ { 0 } ) ^ { \\dagger } = W \\gamma \\gamma _ { 0 } ) ^ { \\dagger } = W \\gamma _ { 0 } ,\n",
      "\n",
      "act : { \\phi } \\rightarrow U \\hat { \\phi } U ^ { \\dagger } .\n",
      "pred: { \\phi } - U \\sqrt { \\phi } U ^ { \\dagger } .\n",
      "\n",
      "act : \\equiv \\chi ^ { - 1 } ~ \\phi ~ \\chi , \\;\n",
      "pred: \\equiv \\chi ^ { - 1 } \\, \\phi \\, \\chi \\, \\chi , \\chi , \\chi ,\n",
      "\n",
      "act : ^ { 9 } \\times S _ { R ^ { 1 1 } } ^ { S S } \\times S _ { R } ^ { 1 }\n",
      "pred: ^ { 3 } \\times S _ { R } ^ { S } { S } } S S _ { R } ^ { S } } S S _ { R } ^ { S } } S\n",
      "\n",
      "act : _ { k } ^ { - p } = ( A _ { k } ^ { p } ) ^ { \\dagger } ;\n",
      "pred: _ { k } ^ { T } ^ { T } = ( A _ { k } ^ { T } ) ^ { \\dagger } ,\n",
      "\n",
      "act : u ^ { \\alpha } , u ^ { \\beta } ] = i \\Theta ^ { \\alpha \\beta } ,\n",
      "pred: u ^ { \\alpha } , u ^ { \\beta } ] = i \\Theta ^ { \\alpha \\beta } , u ^ { \\beta } ,\n",
      "\n",
      "act : | \\chi \\rangle = Q | \\Lambda \\rangle\n",
      "pred: | \\chi \\rangle = Q \\rangle | Q \\rangle \\rangle\n",
      "\n",
      "act : _ { k } \\approx e ^ { - i \\omega t }\n",
      "pred: _ { k } \\approx e ^ { - i \\omega \\alpha }\n",
      "\n",
      "act : \\cal B } _ { 1 } = { \\cal B } _ { 2 } \\rightarrow 0 \\ .\n",
      "pred: \\cal B } _ { 1 } = { \\cal B } _ { 2 } - { \\cal B } _ { 2 } = 0 _ { 2 } = 0 \\ .\n",
      "\n",
      "act : L ^ { \\pm } = L ^ { \\pm } \\dot { \\otimes } L ^ { \\pm }\n",
      "pred: L ^ { \\pm } = L ^ { \\pm } S ^ { \\pm } S ^ { \\pm } = L ^ { \\pm } S ^ { \\pm }\n",
      "\n",
      "act : _ { { \\bf q } } = \\theta ( q ^ { r } ) \\, n ( q ^ { r } ) ,\n",
      "pred: _ { \\mathrm { r } } = \\theta ( q ^ { r } ) \\, n ( q ^ { r } ) \\, n ( q ^ { r } ) \\, n ( q ^ { r\n",
      "\n",
      "act : u \\gg { \\hat { g } } ^ { 1 / 2 } \\gg 1 .\n",
      "pred: u \\geq \\hat { g } ^ { 1 / 2 } \\, \\hat { g } ^ { 1 / 2 } \\geq 1 .\n",
      "\n",
      "act : \\mathcal { S } _ { C S } ( A ) = 0 \\; ,\n",
      "pred: { \\cal S } _ { S S } ( A ) = 0 \\ ,\n",
      "\n",
      "act : ^ { - \\tilde { q } } h ^ { - 1 } 2 ( - \\tilde { q } ) \\upsilon ^ { 2 }\n",
      "pred: ^ { - 1 } 2 ( - \\vec { q } ) ^ { - 1 } 2 } ( - \\vec { q } ) ^ { { - 1 } 2 } h ^ { 2 }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_formula(indices, id2sign):\n",
    "    return ' '.join([id2sign[i.item()] for i in indices if i.item() and i.item() != 2])  # Skip padding\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader):\n",
    "        images, formulas = data\n",
    "        # Pad sequences to the same length\n",
    "        formulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "        targets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "        outputs = model(images, formulas_padded[:, :-1].contiguous())\n",
    "        print(outputs.shape)\n",
    "\n",
    "        predicted_indices = torch.argmax(outputs, dim=2)\n",
    "        print(predicted_indices.shape)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            actual_formula = decode_formula(formulas[i, 1:], id2sign)  # Skip <S> token\n",
    "            predicted_formula = decode_formula(predicted_indices[i, 1:], id2sign)  # Skip <S> token\n",
    "\n",
    "            print('act :', actual_formula)\n",
    "            print('pred:', predicted_formula)\n",
    "            print()\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def predict_formula(model, image, start_token, end_token, max_length, device):\n",
    "    \"\"\"\n",
    "    Generate a LaTeX formula given an image input.\n",
    "\n",
    "    Args:\n",
    "        model: The trained ImageToLaTeXModel.\n",
    "        image: Input image tensor of shape (1, C, H, W).\n",
    "        start_token: Index of the <START> token.\n",
    "        end_token: Index of the <END> token.\n",
    "        max_length: Maximum length of the generated formula.\n",
    "        device: Device to perform inference on.\n",
    "\n",
    "    Returns:\n",
    "        A list of token indices representing the generated formula.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract features using the encoder\n",
    "        features = model.encoder(image)\n",
    "\n",
    "        # Initialize the decoder input with the <START> token\n",
    "        input_token = torch.tensor([[start_token]], device=device)\n",
    "\n",
    "        # Initialize LSTM hidden and cell states\n",
    "        hidden = torch.zeros(1, 1, model.decoder.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, 1, model.decoder.lstm.hidden_size, device=device)\n",
    "\n",
    "        # Store generated tokens\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Embed the current token\n",
    "            embedding = model.decoder.embedding(input_token).squeeze(1)\n",
    "\n",
    "            # Compute attention and context\n",
    "            context, _ = model.decoder.attention(features, hidden.squeeze(0))\n",
    "\n",
    "            # Prepare input for the LSTM\n",
    "            lstm_input = torch.cat((embedding, context), dim=1).unsqueeze(1)\n",
    "\n",
    "            # Forward through LSTM\n",
    "            lstm_out, (hidden, cell) = model.decoder.lstm(lstm_input, (hidden, cell))\n",
    "\n",
    "            # Generate the next token\n",
    "            output = model.decoder.fc(lstm_out.squeeze(1))\n",
    "            next_token = output.argmax(dim=1)\n",
    "\n",
    "            # Append the predicted token\n",
    "            generated_tokens.append(next_token.item())\n",
    "\n",
    "            # Break if <END> token is generated\n",
    "            if next_token.item() == end_token:\n",
    "                break\n",
    "\n",
    "            # Update input token for the next time step\n",
    "            input_token = next_token.unsqueeze(1)\n",
    "\n",
    "    return generated_tokens\n",
    "\n",
    "def decode_formula(indices, id2sign):\n",
    "    return ' '.join([id2sign[i.item()] for i in indices if i.item() and i.item() != 2])  # Skip padding\n",
    "\n",
    "def validate_model(model, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_bleu = 0\n",
    "    total_samples = 0\n",
    "    total_imgs = 0\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, formulas in val_loader:\n",
    "            images, formulas = images.to(device), formulas.to(device)\n",
    "            print(predict_formula(model, images[0], 0, 2, 100, device))\n",
    "            formulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "            targets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "            outputs = model(images, [])\n",
    "            # Match target size with output size\n",
    "            targets = targets[:, :outputs.size(1)].contiguous()\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            predicted_indices = torch.argmax(outputs, dim=2)  # Get the index of the max log-probability\n",
    "            total_samples += formulas[:, 1:].numel()  # Total number of tokens in the validation batch\n",
    "\n",
    "            # Print images and predictions\n",
    "            # The batch size is 400, print every 400 images\n",
    "            total_imgs += len(images)\n",
    "            for i in range(len(images)):\n",
    "                # Decode the actual and predicted formulas\n",
    "                actual_formula = decode_formula(formulas[i, :], id2sign)  \n",
    "                predicted_formula = decode_formula(predicted_indices[i, :], id2sign) \n",
    "                total_bleu += sentence_bleu([actual_formula.split()], predicted_formula.split(), smoothing_function=smooth_fn)\n",
    "                # print(total_bleu)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if i == 0:\n",
    "                    print(f'Actual Formula:    {actual_formula}')\n",
    "                    print(f'Predicted Formula: {predicted_formula}')\n",
    "                    print('-' * 50)\n",
    "\n",
    "    avg_loss = total_loss / total_imgs\n",
    "    accuracy = total_bleu / total_imgs\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# val_loss, val_accuracy = validate_model(model, criterion, device)\n",
    "# print(f'val_accuracy: {val_accuracy*100:.2f}% , val_loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
