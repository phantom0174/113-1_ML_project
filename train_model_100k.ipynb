{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Ti\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(device)\n",
    "# print(len(os.listdir('LaTex_data/split_1')))\n",
    "# print(image_formula_mapping['0002475406d9932.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # do not resize\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "label_to_index_file = './230k.json'\n",
    "with open(label_to_index_file, 'r') as f:\n",
    "\tsign2id = json.load(f)\n",
    "\n",
    "id2sign = [0] * 650\n",
    "for k, v in sign2id.items():\n",
    "\tid2sign[int(v)] = k\n",
    "\n",
    "def collate_fn(batch):\n",
    "\t# filter the pictures that have different weight or height\n",
    "\tsize = batch[0][0].size()\n",
    "\tbatch = [img_formula for img_formula in batch\n",
    "\t\t\tif img_formula[0].size() == size]\n",
    "\t\n",
    "\t# # sort by the length of formula\n",
    "\t# batch.sort(key=lambda img_formula: len(img_formula[1].split()),\n",
    "\t# \t\treverse=True)\n",
    "\n",
    "\timgs, formulas = zip(*batch)\n",
    "\tformulas = pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "\t\n",
    "\timgs = torch.stack(imgs, dim=0)\n",
    "\treturn imgs.to(device), formulas.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join\n",
    "\n",
    "class Im2LatexDataset(Dataset):\n",
    "\tdef __init__(self, data_dir, split, max_len=30000):\n",
    "\t\t\"\"\"args:\n",
    "\t\tdata_dir: root dir storing the prepoccessed data\n",
    "\t\tsplit: train, validate or test\n",
    "\t\t\"\"\"\n",
    "\t\tassert split in [\"train\", \"validate\", \"test\"]\n",
    "\t\tself.data_dir = data_dir\n",
    "\t\tself.split = split\n",
    "\t\tself.max_len = max_len\n",
    "\t\tself.pairs = self._load_pairs()\n",
    "\n",
    "\tdef _load_pairs(self):\n",
    "\t\tpairs = torch.load(join(self.data_dir, \"{}.pkl\".format(self.split)))\n",
    "\n",
    "\t\tfinite_pairs = []\n",
    "\t\tfor i, (img, formula) in enumerate(pairs):\n",
    "\t\t\tpair = (img, \" \".join(formula.split()))\n",
    "\t\t\tfinite_pairs.append(pair)\n",
    "\n",
    "\t\t\tif i >= self.max_len:\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\treturn finite_pairs\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage, formula = self.pairs[idx]\n",
    "\t\t\n",
    "\t\tformula_tokens = '<S> ' + formula + ' <E> '\n",
    "\t\tformula_tokens = formula.split()\n",
    "\t\t\n",
    "\t\tformula_indices = []\n",
    "\t\tfor token in formula_tokens:\n",
    "\t\t\t# Map each token to its index; if not found, use a default index (e.g., 0)\n",
    "\t\t\tindex = sign2id.get(token, 0)  # Assuming 0 is for unknown tokens\n",
    "\t\t\tformula_indices.append(int(index))\n",
    "\t\t\n",
    "\t\treturn image, torch.tensor(formula_indices, dtype=torch.long)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\tIm2LatexDataset('./100k/', 'train'),\n",
    "\tbatch_size=batch_size,\n",
    "\tcollate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\tIm2LatexDataset('./100k/', 'validate'),\n",
    "    batch_size=400,\n",
    "\tcollate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          ...,\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816]],\n",
      "\n",
      "         [[0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          ...,\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911]],\n",
      "\n",
      "         [[0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          ...,\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352]]],\n",
      "\n",
      "\n",
      "        [[[0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          ...,\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385]],\n",
      "\n",
      "         [[0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          ...,\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478]],\n",
      "\n",
      "         [[0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          ...,\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819]]],\n",
      "\n",
      "\n",
      "        [[[0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          ...,\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088]],\n",
      "\n",
      "         [[0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          ...,\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678]],\n",
      "\n",
      "         [[0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          ...,\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          ...,\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603]],\n",
      "\n",
      "         [[0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          ...,\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359]],\n",
      "\n",
      "         [[0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          ...,\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          ...,\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231]],\n",
      "\n",
      "         [[0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          ...,\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233]],\n",
      "\n",
      "         [[0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          ...,\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695]]]],\n",
      "       device='cuda:0'), tensor([[572, 543, 575,  ...,   2,   2,   2],\n",
      "        [287,  43, 543,  ...,   2,   2,   2],\n",
      "        [ 83, 543, 575,  ..., 557, 577,  71],\n",
      "        ...,\n",
      "        [561, 542, 575,  ...,   2,   2,   2],\n",
      "        [575, 166,  51,  ...,   2,   2,   2],\n",
      "        [  6, 382, 563,  ...,   2,   2,   2]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    img, label = data\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder / Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_out_dim, dec_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(enc_out_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        B, seq_len, enc_dim = encoder_outputs.shape\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, hidden), dim=2)))\n",
    "        energy = energy @ (self.v / torch.sqrt(torch.tensor(enc_dim, dtype=torch.float)))\n",
    "        attn_weights = F.softmax(energy, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, enc_out_dim=512, dropout_prob=0):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "\n",
    "\n",
    "            nn.Conv2d(256, enc_out_dim, 3, 1, 0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn_encoder(images)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        B, H, W, C = features.shape\n",
    "        features = features.contiguous().view(B, H * W, C)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, enc_out_dim, dropout_prob=0.3):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = Attention(enc_out_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(enc_out_dim, hidden_dim, batch_first=True)  # LSTM only takes features\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, features, formulas):\n",
    "        hidden = torch.zeros(1, features.size(0), self.lstm.hidden_size, device=features.device)\n",
    "        cell = torch.zeros(1, features.size(0), self.lstm.hidden_size, device=features.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(formulas.size(1)):\n",
    "            # Compute attention context vector for the features\n",
    "            context, _ = self.attention(features, hidden.squeeze(0))\n",
    "\n",
    "            # LSTM processes the context vector at each time step\n",
    "            context = context.unsqueeze(1)  # Add time dimension\n",
    "            lstm_out, (hidden, cell) = self.lstm(context, (hidden, cell))\n",
    "\n",
    "            # Generate token probabilities\n",
    "            outputs.append(self.fc(lstm_out.squeeze(1)))\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "class ImageToLaTeXModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageToLaTeXModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, formulas):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, formulas[:, :-1])\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_state(model, optimizer, epoch, loss):\n",
    "\tstate = {\n",
    "\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t'epoch': epoch,\n",
    "\t\t'loss': loss\n",
    "\t}\n",
    "\ttorch.save(state, 'model_checkpoint.pth')\n",
    "\n",
    "# Function to load model state\n",
    "def load_training_state(model, optimizer):\n",
    "\tcheckpoint = torch.load('model_checkpoint.pth')\n",
    "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
    "\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\treturn checkpoint['epoch'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model found, starting fresh.\n",
      "Epoch [1/1000], Step [0/938], Loss: 6.3598\n",
      "Epoch [1/1000], Step [100/938], Loss: 2.0194\n",
      "Epoch [1/1000], Step [200/938], Loss: 2.7715\n",
      "Epoch [1/1000], Step [300/938], Loss: 2.9531\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBED_SIZE = 512 # direct output dim from cv_tiny\n",
    "\n",
    "hidden_size = 1024\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.003\n",
    "\n",
    "vocab_size = len(sign2id)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "encoder = EncoderCNN(EMBED_SIZE).to(device)\n",
    "decoder = DecoderRNN(EMBED_SIZE, hidden_size, vocab_size,512).to(device)\n",
    "model = ImageToLaTeXModel(encoder, decoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "start_epoch = 0\n",
    "\n",
    "# Try to resume from a checkpoint\n",
    "try:\n",
    "\tstart_epoch, last_loss = load_training_state(model, optimizer)\n",
    "\tprint(f\"Get model from epoch {start_epoch}, with loss {last_loss:.4f}\")\n",
    "except FileNotFoundError:\n",
    "\tprint(\"No saved model found, starting fresh.\")\n",
    "\tstart_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "\t\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\timages, formulas = data\n",
    "\t\t# Pad sequences to the same length\n",
    "\t\tformulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "\t\ttargets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "\t\toutputs = model(images, formulas_padded[:, :-1].contiguous())\n",
    "\t\t# Match target size with output size\n",
    "\t\ttargets = targets[:, :outputs.size(1)].contiguous()\n",
    "\n",
    "\t\tloss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif i  % 100 == 0:\n",
    "\t\t\tprint(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\tsave_training_state(model, optimizer, epoch, loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 109\u001b[0m\n\u001b[0;32m    105\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m total_bleu \u001b[38;5;241m/\u001b[39m total_imgs\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, accuracy\n\u001b[1;32m--> 109\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% , val_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 77\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, criterion, device)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, formulas \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     76\u001b[0m     images, formulas \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), formulas\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mpredict_formula\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     78\u001b[0m     formulas_padded \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(formulas, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     79\u001b[0m     targets \u001b[38;5;241m=\u001b[39m formulas_padded[:, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "Cell \u001b[1;32mIn[18], line 23\u001b[0m, in \u001b[0;36mpredict_formula\u001b[1;34m(model, image, start_token, end_token, max_length, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Extract features using the encoder\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Initialize the decoder input with the <START> token\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     input_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[start_token]], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\bibby\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m, in \u001b[0;36mEncoderCNN.forward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[0;32m     49\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_encoder(images)\n\u001b[1;32m---> 50\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     B, H, W, C \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     52\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, H \u001b[38;5;241m*\u001b[39m W, C)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "def predict_formula(model, image, start_token, end_token, max_length, device):\n",
    "    \"\"\"\n",
    "    Generate a LaTeX formula given an image input.\n",
    "\n",
    "    Args:\n",
    "        model: The trained ImageToLaTeXModel.\n",
    "        image: Input image tensor of shape (1, C, H, W).\n",
    "        start_token: Index of the <START> token.\n",
    "        end_token: Index of the <END> token.\n",
    "        max_length: Maximum length of the generated formula.\n",
    "        device: Device to perform inference on.\n",
    "\n",
    "    Returns:\n",
    "        A list of token indices representing the generated formula.\n",
    "    \"\"\"\n",
    "    print(image)\n",
    "    model.eval()\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract features using the encoder\n",
    "        features = model.encoder(image)\n",
    "\n",
    "        # Initialize the decoder input with the <START> token\n",
    "        input_token = torch.tensor([[start_token]], device=device)\n",
    "\n",
    "        # Initialize LSTM hidden and cell states\n",
    "        hidden = torch.zeros(1, 1, model.decoder.lstm.hidden_size, device=device)\n",
    "        cell = torch.zeros(1, 1, model.decoder.lstm.hidden_size, device=device)\n",
    "\n",
    "        # Store generated tokens\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            context, _ = model.decoder.attention(features, hidden.squeeze(0))\n",
    "            lstm_input = torch.cat((embeddings[:, t, :], context), dim=1).unsqueeze(1)\n",
    "            lstm_out, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            outputs.append(self.fc(lstm_out.squeeze(1)))\n",
    "\n",
    "    return generated_tokens\n",
    "\n",
    "def decode_formula(indices, id2sign):\n",
    "    return ' '.join([id2sign[i.item()] for i in indices if i.item() and i.item() != 2])  # Skip padding\n",
    "\n",
    "def validate_model(model, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_bleu = 0\n",
    "    total_samples = 0\n",
    "    total_imgs = 0\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, formulas in val_loader:\n",
    "            images, formulas = images.to(device), formulas.to(device)\n",
    "            print(predict_formula(model, images[0], 0, 1, 100, device))\n",
    "            formulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "            targets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "            outputs = model(images, [])\n",
    "            # Match target size with output size\n",
    "            targets = targets[:, :outputs.size(1)].contiguous()\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            predicted_indices = torch.argmax(outputs, dim=2)  # Get the index of the max log-probability\n",
    "            total_samples += formulas[:, 1:].numel()  # Total number of tokens in the validation batch\n",
    "\n",
    "            # Print images and predictions\n",
    "            # The batch size is 400, print every 400 images\n",
    "            total_imgs += len(images)\n",
    "            for i in range(len(images)):\n",
    "                # Decode the actual and predicted formulas\n",
    "                actual_formula = decode_formula(formulas[i, :], id2sign)  \n",
    "                predicted_formula = decode_formula(predicted_indices[i, :], id2sign) \n",
    "                total_bleu += sentence_bleu([actual_formula.split()], predicted_formula.split(), smoothing_function=smooth_fn)\n",
    "                # print(total_bleu)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if i == 0:\n",
    "                    print(f'Actual Formula:    {actual_formula}')\n",
    "                    print(f'Predicted Formula: {predicted_formula}')\n",
    "                    print('-' * 50)\n",
    "\n",
    "    avg_loss = total_loss / total_imgs\n",
    "    accuracy = total_bleu / total_imgs\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "val_loss, val_accuracy = validate_model(model, criterion, device)\n",
    "print(f'val_accuracy: {val_accuracy*100:.2f}% , val_loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
