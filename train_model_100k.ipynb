{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4060 Ti\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(device)\n",
    "# print(len(os.listdir('LaTex_data/split_1')))\n",
    "# print(image_formula_mapping['0002475406d9932.png'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # do not resize\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "label_to_index_file = './230k.json'\n",
    "with open(label_to_index_file, 'r') as f:\n",
    "\tsign2id = json.load(f)\n",
    "\n",
    "id2sign = [0] * 650\n",
    "for k, v in sign2id.items():\n",
    "\tid2sign[int(v)] = k\n",
    "\n",
    "def collate_fn(batch):\n",
    "\t# filter the pictures that have different weight or height\n",
    "\tsize = batch[0][0].size()\n",
    "\tbatch = [img_formula for img_formula in batch\n",
    "\t\t\tif img_formula[0].size() == size]\n",
    "\t\n",
    "\t# # sort by the length of formula\n",
    "\t# batch.sort(key=lambda img_formula: len(img_formula[1].split()),\n",
    "\t# \t\treverse=True)\n",
    "\n",
    "\timgs, formulas = zip(*batch)\n",
    "\tformulas = pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "\t\n",
    "\timgs = torch.stack(imgs, dim=0)\n",
    "\treturn imgs.to(device), formulas.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join\n",
    "\n",
    "class Im2LatexDataset(Dataset):\n",
    "\tdef __init__(self, data_dir, split, max_len=30000):\n",
    "\t\t\"\"\"args:\n",
    "\t\tdata_dir: root dir storing the prepoccessed data\n",
    "\t\tsplit: train, validate or test\n",
    "\t\t\"\"\"\n",
    "\t\tassert split in [\"train\", \"validate\", \"test\"]\n",
    "\t\tself.data_dir = data_dir\n",
    "\t\tself.split = split\n",
    "\t\tself.max_len = max_len\n",
    "\t\tself.pairs = self._load_pairs()\n",
    "\n",
    "\tdef _load_pairs(self):\n",
    "\t\tpairs = torch.load(join(self.data_dir, \"{}.pkl\".format(self.split)))\n",
    "\n",
    "\t\tfinite_pairs = []\n",
    "\t\tfor i, (img, formula) in enumerate(pairs):\n",
    "\t\t\tpair = (img, \" \".join(formula.split()))\n",
    "\t\t\tfinite_pairs.append(pair)\n",
    "\n",
    "\t\t\tif i >= self.max_len:\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\treturn finite_pairs\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\timage, formula = self.pairs[idx]\n",
    "\t\t\n",
    "\t\tformula_tokens = '<S> ' + formula + ' <E> '\n",
    "\t\tformula_tokens = formula.split()\n",
    "\t\t\n",
    "\t\tformula_indices = []\n",
    "\t\tfor token in formula_tokens:\n",
    "\t\t\t# Map each token to its index; if not found, use a default index (e.g., 0)\n",
    "\t\t\tindex = sign2id.get(token, 0)  # Assuming 0 is for unknown tokens\n",
    "\t\t\tformula_indices.append(int(index))\n",
    "\t\t\n",
    "\t\treturn image, torch.tensor(formula_indices, dtype=torch.long)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\tIm2LatexDataset('./100k/', 'train'),\n",
    "\tbatch_size=batch_size,\n",
    "\tcollate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\tIm2LatexDataset('./100k/', 'validate'),\n",
    "\tbatch_size=400,\n",
    "\tcollate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          ...,\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816],\n",
      "          [0.6816, 0.6816, 0.6816,  ..., 0.6816, 0.6816, 0.6816]],\n",
      "\n",
      "         [[0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          ...,\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911],\n",
      "          [0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911]],\n",
      "\n",
      "         [[0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          ...,\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352],\n",
      "          [0.7352, 0.7352, 0.7352,  ..., 0.7352, 0.7352, 0.7352]]],\n",
      "\n",
      "\n",
      "        [[[0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          ...,\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385],\n",
      "          [0.7385, 0.7385, 0.7385,  ..., 0.7385, 0.7385, 0.7385]],\n",
      "\n",
      "         [[0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          ...,\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478],\n",
      "          [0.9478, 0.9478, 0.9478,  ..., 0.9478, 0.9478, 0.9478]],\n",
      "\n",
      "         [[0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          ...,\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819],\n",
      "          [0.9819, 0.9819, 0.9819,  ..., 0.9819, 0.9819, 0.9819]]],\n",
      "\n",
      "\n",
      "        [[[0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          ...,\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088],\n",
      "          [0.7088, 0.7088, 0.7088,  ..., 0.7088, 0.7088, 0.7088]],\n",
      "\n",
      "         [[0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          ...,\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678],\n",
      "          [0.9678, 0.9678, 0.9678,  ..., 0.9678, 0.9678, 0.9678]],\n",
      "\n",
      "         [[0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          ...,\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354],\n",
      "          [0.8354, 0.8354, 0.8354,  ..., 0.8354, 0.8354, 0.8354]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          ...,\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603],\n",
      "          [0.6603, 0.6603, 0.6603,  ..., 0.6603, 0.6603, 0.6603]],\n",
      "\n",
      "         [[0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          ...,\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359],\n",
      "          [0.9359, 0.9359, 0.9359,  ..., 0.9359, 0.9359, 0.9359]],\n",
      "\n",
      "         [[0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          ...,\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840],\n",
      "          [0.7840, 0.7840, 0.7840,  ..., 0.7840, 0.7840, 0.7840]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          ...,\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231],\n",
      "          [0.6231, 0.6231, 0.6231,  ..., 0.6231, 0.6231, 0.6231]],\n",
      "\n",
      "         [[0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          ...,\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233],\n",
      "          [0.6233, 0.6233, 0.6233,  ..., 0.6233, 0.6233, 0.6233]],\n",
      "\n",
      "         [[0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          ...,\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695],\n",
      "          [0.7695, 0.7695, 0.7695,  ..., 0.7695, 0.7695, 0.7695]]]],\n",
      "       device='cuda:0'), tensor([[572, 543, 575,  ...,   2,   2,   2],\n",
      "        [287,  43, 543,  ...,   2,   2,   2],\n",
      "        [ 83, 543, 575,  ..., 557, 577,  71],\n",
      "        ...,\n",
      "        [561, 542, 575,  ...,   2,   2,   2],\n",
      "        [575, 166,  51,  ...,   2,   2,   2],\n",
      "        [  6, 382, 563,  ...,   2,   2,   2]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    img, label = data\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(self, images, beam_width=3, max_len=100, start_token=0, end_token=1):\n",
    "    features = self.encoder(images)\n",
    "    B = features.size(0)\n",
    "    hidden = torch.zeros(1, B, self.decoder.gru.hidden_size, device=features.device)\n",
    "\n",
    "    sequences = [[(start_token, 0.0)]] * B  # Each batch starts with <SOS>\n",
    "    completed_sequences = [[] for _ in range(B)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = [[] for _ in range(B)]\n",
    "        for b in range(B):\n",
    "            if sequences[b][-1][0] == end_token:  # If sequence ends\n",
    "                completed_sequences[b].append(sequences[b])\n",
    "                continue\n",
    "\n",
    "            input_token = torch.tensor([sequences[b][-1][0]], device=features.device).unsqueeze(0)\n",
    "            embedding = self.decoder.embedding(input_token).squeeze(1)\n",
    "            context, _ = self.decoder.attention(features[b:b+1], hidden[:, b:b+1, :].squeeze(1))\n",
    "            gru_input = torch.cat((embedding, context), dim=1).unsqueeze(1)\n",
    "            gru_out, hidden[:, b:b+1, :] = self.decoder.gru(gru_input, hidden[:, b:b+1, :])\n",
    "            logits = self.decoder.fc(gru_out.squeeze(1))\n",
    "            probs = torch.log_softmax(logits, dim=1)\n",
    "\n",
    "            for token_idx in range(len(probs[0])):\n",
    "                prob = probs[0, token_idx].item()\n",
    "                new_seq = sequences[b] + [(token_idx, prob)]\n",
    "                all_candidates[b].append((new_seq, sum(prob for _, prob in new_seq)))\n",
    "\n",
    "        # Prune sequences\n",
    "        for b in range(B):\n",
    "            all_candidates[b].sort(key=lambda x: x[1], reverse=True)\n",
    "            sequences[b] = all_candidates[b][:beam_width]\n",
    "\n",
    "    # Return best sequences\n",
    "    return [[token for token, _ in seq[:-1]] for seq in completed_sequences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder / Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_out_dim, dec_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(enc_out_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        B, seq_len, enc_dim = encoder_outputs.shape\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, hidden), dim=2)))\n",
    "        energy = energy @ (self.v / torch.sqrt(torch.tensor(enc_dim, dtype=torch.float)))\n",
    "        attn_weights = F.softmax(energy, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, enc_out_dim=512, dropout_prob=0):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "\n",
    "\n",
    "            nn.Conv2d(256, enc_out_dim, 3, 1, 0),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.cnn_encoder(images)\n",
    "        features = features.permute(0, 2, 3, 1)\n",
    "        B, H, W, C = features.shape\n",
    "        features = features.contiguous().view(B, H * W, C)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, enc_out_dim, dropout_prob=0.3):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = Attention(enc_out_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + enc_out_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, features, formulas):\n",
    "        embeddings = self.dropout(self.embedding(formulas))\n",
    "        \n",
    "        # Initialize LSTM hidden and cell states\n",
    "        hidden = torch.zeros(1, features.size(0), self.lstm.hidden_size, device=features.device)\n",
    "        cell = torch.zeros(1, features.size(0), self.lstm.hidden_size, device=features.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(embeddings.size(1)):\n",
    "            context, _ = self.attention(features, hidden.squeeze(0))\n",
    "            lstm_input = torch.cat((embeddings[:, t, :], context), dim=1).unsqueeze(1)\n",
    "            lstm_out, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "            outputs.append(self.fc(lstm_out.squeeze(1)))\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "class ImageToLaTeXModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ImageToLaTeXModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, images, formulas):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, formulas[:, :-1])\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_state(model, optimizer, epoch, loss):\n",
    "\tstate = {\n",
    "\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
    "\t\t'epoch': epoch,\n",
    "\t\t'loss': loss\n",
    "\t}\n",
    "\ttorch.save(state, 'model_checkpoint.pth')\n",
    "\n",
    "# Function to load model state\n",
    "def load_training_state(model, optimizer):\n",
    "\tcheckpoint = torch.load('model_checkpoint.pth')\n",
    "\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
    "\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\treturn checkpoint['epoch'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get model from epoch 22, with loss 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EMBED_SIZE = 512 # direct output dim from cv_tiny\n",
    "\n",
    "hidden_size = 1024\n",
    "num_epochs = 22\n",
    "learning_rate = 0.003\n",
    "\n",
    "vocab_size = len(sign2id)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "encoder = EncoderCNN(EMBED_SIZE).to(device)\n",
    "decoder = DecoderRNN(EMBED_SIZE, hidden_size, vocab_size,512).to(device)\n",
    "model = ImageToLaTeXModel(encoder, decoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "start_epoch = 0\n",
    "\n",
    "# Try to resume from a checkpoint\n",
    "try:\n",
    "\tstart_epoch, last_loss = load_training_state(model, optimizer)\n",
    "\tprint(f\"Get model from epoch {start_epoch}, with loss {last_loss:.4f}\")\n",
    "except FileNotFoundError:\n",
    "\tprint(\"No saved model found, starting fresh.\")\n",
    "\tstart_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "\t\n",
    "\tfor i, data in enumerate(train_loader):\n",
    "\t\timages, formulas = data\n",
    "\t\t# Pad sequences to the same length\n",
    "\t\tformulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "\t\ttargets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "\t\toutputs = model(images, formulas_padded[:, :-1].contiguous())\n",
    "\t\t# Match target size with output size\n",
    "\t\ttargets = targets[:, :outputs.size(1)].contiguous()\n",
    "\n",
    "\t\tloss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif i  % 100 == 0:\n",
    "\t\t\tprint(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\tsave_training_state(model, optimizer, epoch, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Formula:    j _ { 1 } ^ { k } = \\omega _ { 1 } ^ { k - 2 } \\subseteq \\omega _ { 1 } ^ { k }\n",
      "Predicted Formula: j _ { 1 } ^ { k } = \\omega _ { 1 } ^ { k - 2 } \\subseteq \\omega _ { 1 } ^ { k }\n",
      "--------------------------------------------------\n",
      "Actual Formula:    \\Omega _ { \\vec { n } } H ^ { \\prime } \\Omega _ { \\vec { n } } ^ { \\dagger } = H ^ { \\prime } \\ ,\n",
      "Predicted Formula: \\Omega _ { \\vec { n } } H ^ { \\prime } \\Omega _ { \\vec { n } } ^ { \\dagger } = H ^ { \\prime } \\ ,\n",
      "--------------------------------------------------\n",
      "Actual Formula:    { \\cal D } A { \\cal D } E = { \\cal D } C { \\cal D } { \\cal E }\n",
      "Predicted Formula: { \\cal D } A { \\cal D } E = { \\cal D } C { \\cal D } { \\cal E }\n",
      "--------------------------------------------------\n",
      "Actual Formula:    \\psi ( t ) = e ^ { - i \\int _ { 0 } ^ { t } d t ^ { \\prime } \\, A ( t ^ { \\prime } ) } \\, \\tilde { \\psi } ( t )\n",
      "Predicted Formula: \\psi ( t ) = e ^ { - i \\int _ { 0 } ^ { t } d t ^ { \\prime } \\, A ( t ^ { \\prime } ) } \\, \\tilde { \\psi } ( t )\n",
      "--------------------------------------------------\n",
      "Actual Formula:    \\delta J ^ { a } = f ^ { a b c } w _ { b } g _ { c d } J ^ { d } + 2 g ^ { a b } \\partial _ { + } w _ { b } ,\n",
      "Predicted Formula: \\delta J ^ { a } = f ^ { a b c } w _ { b } g _ { c d } J ^ { d } + 2 g ^ { a b } \\partial _ { + } w _ { b } ,\n",
      "--------------------------------------------------\n",
      "Actual Formula:    U _ { ( x _ { 1 } , x _ { 2 } ) } \\ \\rightarrow \\, G a m m a _ { 1 } ^ { x _ { 1 } } \\Gamma _ { 2 } ^ { x _ { 2 } } U ( \\Gamma _ { 2 } ^ { } ) ^ { x _ { 2 } } ( \\Gamma _ { 1 } ^ { } ) ^ { x _ { 1 } } ,\n",
      "Predicted Formula: U _ { ( x _ { 1 } , x _ { 2 } ) } \\ \\rightarrow \\, G a m m a _ { 1 } ^ { x _ { 1 } } \\Gamma _ { 2 } ^ { x _ { 2 } } U ( \\Gamma _ { 2 } ^ { } ) ^ { x _ { 2 } } ( \\Gamma _ { 1 } ^ { } ) ^ { x _ { 1 } } ,\n",
      "--------------------------------------------------\n",
      "Actual Formula:    { \\cal D } { \\bar { \\psi } } { \\cal D } \\psi \\rightarrow { \\cal D } { \\bar { \\psi } } { \\cal D } \\psi \\, \\, \\, { \\displaystyle e } ^ { \\frac { i } { \\pi } \\int d x _ { + } d x _ { - } S \\partial _ { + } \\partial _ { - } S } .\n",
      "Predicted Formula: { \\cal D } { \\bar { \\psi } } { \\cal D } \\psi \\rightarrow { \\cal D } { \\bar { \\psi } } { \\cal D } \\psi \\, \\, \\, { \\displaystyle e } ^ { \\frac { i } { \\pi } \\int d x _ { + } d x _ { - } S \\partial _ { + } \\partial _ { - } S } .\n",
      "--------------------------------------------------\n",
      "Actual Formula:    \\mathcal { T } _ { A } ( H ) : = \\left\\{ F \\in D ( H ) | F ( x k , y ) = F ( x , y ) \\forall k \\in K _ { A } \\right\\} .\n",
      "Predicted Formula: \\mathcal { T } _ { A } ( H ) : = \\left\\{ F \\in D ( H ) | F ( x k , y ) = F ( x , y ) \\forall k \\in K _ { A } \\right\\} .\n",
      "--------------------------------------------------\n",
      "Actual Formula:    f ( t ) = { \\frac { 1 } { 2 } } a _ { 1 } a _ { 2 } a _ { 3 } ,\n",
      "Predicted Formula: f ( t ) = { \\frac { 1 } { 2 } } a _ { 1 } a _ { 2 } a _ { 3 } ,\n",
      "--------------------------------------------------\n",
      "Actual Formula:    \\varphi ( n ) = \\frac { { s i n h } \\alpha n } { { s i n h } \\alpha } \\ \\ ,\n",
      "Predicted Formula: \\varphi ( n ) = \\frac { { s i n h } \\alpha n } { { s i n h } \\alpha } \\ \\ ,\n",
      "--------------------------------------------------\n",
      "Actual Formula:    W [ A ] = \\int \\! d ^ { 3 } x \\, \\frac { 1 } { 2 } B ( x ) ^ { 2 } \\, .\n",
      "Predicted Formula: W [ A ] = \\int \\! d ^ { 3 } x \\, \\frac { 1 } { 2 } B ( x ) ^ { 2 } \\, .\n",
      "--------------------------------------------------\n",
      "Actual Formula:    \\tau ^ { c \\dot { \\alpha } } { } _ { \\dot { \\beta } } ( \\bar { a } ^ { \\dot { \\beta } } a _ { \\dot { \\alpha } } ) _ { i j } = \\delta _ { i j } \\delta ^ { c 3 } \\zeta .\n",
      "Predicted Formula: \\tau ^ { c \\dot { \\alpha } } { } _ { \\dot { \\beta } } ( \\bar { a } ^ { \\dot { \\beta } } a _ { \\dot { \\alpha } } ) _ { i j } = \\delta _ { i j } \\delta ^ { c 3 } \\zeta .\n",
      "--------------------------------------------------\n",
      "Actual Formula:    f ( L _ { i } ) = \\sum _ { i _ { 1 } , . . . , i _ { k } } { \\alpha } _ { i _ { 1 } , . . . , i _ { k } } L _ { i _ { 1 } } . . . L _ { i _ { k } } .\n",
      "Predicted Formula: f ( L _ { i } ) = \\sum _ { i _ { 1 } , . . . , i _ { k } } { \\alpha } _ { i _ { 1 } , . . . , i _ { k } } L _ { i _ { 1 } } . . . L _ { i _ { k } } .\n",
      "--------------------------------------------------\n",
      "Actual Formula:    S _ { e f f } = - { \\frac { \\kappa } { 2 } } \\, \\int d ^ { 2 } \\sigma \\, \\sum _ { k \\not = n } P _ { + } ^ { k } \\, \\partial _ { - } v _ { k } \\ ,\n",
      "Predicted Formula: S _ { e f f } = - { \\frac { \\kappa } { 2 } } \\, \\int d ^ { 2 } \\sigma \\, \\sum _ { k \\not = n } P _ { + } ^ { k } \\, \\partial _ { - } v _ { k } \\ ,\n",
      "--------------------------------------------------\n",
      "Actual Formula:    v _ { n } = \\frac { 1 } { 2 \\sqrt { \\tilde { n } } } e ^ { - i \\tilde { n } \\tau } , \\qquad \\tilde { v } _ { n } = \\frac { 1 } { 2 \\sqrt { \\tilde { n } } } e ^ { i \\tilde { n } \\tau }\n",
      "Predicted Formula: v _ { n } = \\frac { 1 } { 2 \\sqrt { \\tilde { n } } } e ^ { - i \\tilde { n } \\tau } , \\qquad \\tilde { v } _ { n } = \\frac { 1 } { 2 \\sqrt { \\tilde { n } } } e ^ { i \\tilde { n } \\tau }\n",
      "--------------------------------------------------\n",
      "Actual Formula:    g ^ { 2 } \\rightarrow \\tilde { g } ^ { 2 } = { \\frac { g ^ { 2 } Q } { \\sum _ { b = 0 } ^ { K - 1 } \\tilde { B } ( b ) } } = { \\frac { g ^ { 2 } \\sum _ { a = 0 } ^ { K - 1 } B ( a ) } { P } } .\n",
      "Predicted Formula: g ^ { 2 } \\rightarrow \\tilde { g } ^ { 2 } = { \\frac { g ^ { 2 } Q } { \\sum _ { b = 0 } ^ { K - 1 } \\tilde { B } ( b ) } } = { \\frac { g ^ { 2 } \\sum _ { a = 0 } ^ { K - 1 } B ( a ) } { P } } .\n",
      "--------------------------------------------------\n",
      "Actual Formula:    \\{ { A _ { i } ^ { a } } ( x ) , { A ^ { b } } _ { j } ( y ) \\} = { \\epsilon _ { a b } } { \\epsilon _ { i j k } } { \\frac { \\partial _ { k } } { \\bf \\nabla ^ { 2 } } } \\delta ( x - y )\n",
      "Predicted Formula: \\{ { A _ { i } ^ { a } } ( x ) , { A ^ { b } } _ { j } ( y ) \\} = { \\epsilon _ { a b } } { \\epsilon _ { i j k } } { \\frac { \\partial _ { k } } { \\bf \\nabla ^ { 2 } } } \\delta ( x - y )\n",
      "--------------------------------------------------\n",
      "Actual Formula:    { \\displaystyle \\int } { \\cal D } y \\, \\mathrm { e x p } \\Bigl [ - \\int _ { 0 } ^ { T } d \\tau { \\frac { 1 } { 4 } } { \\dot { y } } ^ { 2 } \\Bigr ] = \\mathrm { D e t ^ { \\prime } } _ { P } ^ { - { \\frac { 1 } { 2 } } } \\bigl [ - { \\partial } _ { \\tau } ^ { 2 } \\bigr ] = { \\lbrack 4 \\pi T \\rbrack } ^ { - { \\frac { D } { 2 } } } \\,\n",
      "Predicted Formula: { \\displaystyle \\int } { \\cal D } y \\, \\mathrm { e x p } \\bigr [ - \\int _ { 0 } ^ { T } d \\tau { \\frac { 1 } { 4 } } { \\dot { y } } ^ { 2 } \\Bigr ] = \\mathrm { D e t ^ { \\prime } } _ { P } ^ { - { \\frac { 1 } { 2 } } } \\bigl [ - { \\partial } _ { \\tau } ^ { 2 } \\bigr ] = { \\lbrack 4 \\pi T \\rbrack } ^ { - { \\frac { D } { 2 } } } \\,\n",
      "--------------------------------------------------\n",
      "Actual Formula:    { \\cal A } _ { f i } ^ { ( 1 ) } = \\int d { \\bf x \\, } \\mathrm { e } ^ { \\, - i \\, { \\bf p } ^ { ^ { \\prime } } { \\bf \\cdot \\, x } } \\, \\left[ \\frac { m } { w _ { { \\bf p } } } \\, H _ { 1 } ( { \\bf x } , { \\bf \\nabla } ) + V ( { \\bf x ) } \\right] \\, \\mathrm { e } ^ { \\, i \\, { \\bf p \\, \\cdot \\, x } } \\; .\n",
      "Predicted Formula: { \\cal A } _ { f i } ^ { ( 1 ) } = \\int d { \\bf x \\, } \\mathrm { e } ^ { \\, - i \\, { \\bf p } ^ { ^ { \\prime } } { \\bf \\cdot \\, x } } \\, \\left[ \\frac { m } { w _ { { \\bf p } } } \\, H _ { 1 } ( { \\bf x } , { \\bf \\nabla } ) + V ( { \\bf x ) } \\right] \\, \\mathrm { e } ^ { \\, i \\, { \\bf p \\, \\cdot \\, x } } \\; .\n",
      "--------------------------------------------------\n",
      "Actual Formula:    F ( x ^ { + } , x ^ { - } , 0 ) = \\frac { 1 } { 2 \\pi } \\sum _ { K , N _ { \\perp } , \\alpha } \\frac { 1 } { 2 L } \\frac { 1 } { l } \\left( \\frac { \\pi K } { L } \\right) ^ { 3 } e ^ { - i P _ { \\alpha } ^ { - } x ^ { + } - i P ^ { + } x ^ { - } } \\frac { | \\langle u | \\alpha \\rangle | ^ { 2 } } { l K ^ { 3 } | N _ { u } | ^ { 2 } } \\, .\n",
      "Predicted Formula: F ( x ^ { + } , x ^ { - } , 0 ) = \\frac { 1 } { 2 \\pi } \\sum _ { K , N _ { \\perp } , \\alpha } \\frac { 1 } { 2 L } \\frac { 1 } { l } \\left( \\frac { \\pi K } { L } \\right) ^ { 3 } e ^ { - i P _ { \\alpha } ^ { - } x ^ { + } - i P ^ { + } x ^ { - } } \\frac { | \\langle u | \\alpha \\rangle | ^ { 2 } } { l K ^ { 3 } | N _ { u } | ^ { 2 } } \\, .\n",
      "--------------------------------------------------\n",
      "Actual Formula:    d s _ { 5 } ^ { 2 } = \\Big [ 1 - ( { \\frac { \\mu } { r } } ) ^ { 2 } \\Big ] ^ { \\frac { 1 } { 3 } } \\Big [ - ( 1 - ( { \\frac { \\mu } { r } } ) ^ { 2 } ) d t ^ { 2 } + ( 1 - ( { \\frac { \\mu } { r } } ) ^ { 2 } ) ^ { - 2 } d r ^ { 2 } + r ^ { 2 } d \\Omega _ { 3 } ^ { 2 } \\Big ] .\n",
      "Predicted Formula: d s _ { 5 } ^ { 2 } = \\Big [ 1 - ( { \\frac { \\mu } { r } } ) ^ { 2 } \\Big ] ^ { \\frac { 1 } { 3 } } \\Big [ - ( 1 - ( { \\frac { \\mu } { r } } ) ^ { 2 } ) d t ^ { 2 } + ( 1 - ( { \\frac { \\mu } { r } } ) ^ { 2 } ) ^ { - 2 } d r ^ { 2 } + r ^ { 2 } d \\Omega _ { 3 } ^ { 2 } \\Big ] .\n",
      "--------------------------------------------------\n",
      "val_accuracy: 0.0000, val_loss: 0.0025\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def decode_formula(indices, id2sign):\n",
    "    return ' '.join([id2sign[i.item()] for i in indices if i.item() and i.item() != 2])  # Skip padding\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_bleu = 0\n",
    "    total_samples = 0\n",
    "    total_imgs = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, formulas in val_loader:\n",
    "            images, formulas = images.to(device), formulas.to(device)\n",
    "            formulas_padded = nn.utils.rnn.pad_sequence(formulas, batch_first=True, padding_value=2)\n",
    "            targets = formulas_padded[:, :].contiguous()\n",
    "\n",
    "            outputs = model(images, formulas_padded[:, :].contiguous())\n",
    "            # Match target size with output size\n",
    "            targets = targets[:, :outputs.size(1)].contiguous()\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            predicted_indices = torch.argmax(outputs, dim=2)  # Get the index of the max log-probability\n",
    "            total_samples += formulas[:, 1:].numel()  # Total number of tokens in the validation batch\n",
    "\n",
    "            # Print images and predictions\n",
    "            total_imgs += len(images)\n",
    "            for i in range(len(images)):\n",
    "                # Decode the actual and predicted formulas\n",
    "                actual_formula = decode_formula(formulas[i, :], id2sign)  \n",
    "                predicted_formula = decode_formula(predicted_indices[i, :], id2sign) \n",
    "                total_bleu += sentence_bleu(actual_formula, predicted_formula)\n",
    "                total_loss += loss.item()\n",
    "                # print(f'Image: {images[i]}')  # This will print the tensor, consider using visualization instead\n",
    "                if i == 0:\n",
    "                    print(f'Actual Formula:    {actual_formula}')\n",
    "                    print(f'Predicted Formula: {predicted_formula}')\n",
    "                    print('-' * 50)\n",
    "\n",
    "    avg_loss = total_loss / total_imgs\n",
    "    accuracy = total_bleu / total_imgs\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "# Assuming you have your model, dataloader, criterion, and device set up\n",
    "# Assuming 230k.json is loaded as label_to_index\n",
    "\n",
    "val_loss, val_accuracy = validate_model(model, criterion, device)\n",
    "print(f'val_accuracy: {val_accuracy:.4f}, val_loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
